{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcac Gi\u1ea3i ngh\u0129a ti\u1ebfng l\u00f3ng Gen Z Vi\u1ec7t b\u1eb1ng m\u00f4 h\u00ecnh Qwen 1.5 Chat\n", "Phi\u00ean b\u1ea3n n\u00e0y ch\u1ea1y ho\u00e0n to\u00e0n tr\u00ean Google Colab, s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh **Qwen 1.5-0.5B-Chat**, kh\u00f4ng c\u1ea7n GPU, kh\u00f4ng c\u1ea7n API key.\n", "- T\u00e1ch th\u00e0nh t\u1eebng \u00f4 (cell) r\u00f5 r\u00e0ng\n", "- Prompt \u0111\u01b0\u1ee3c t\u1ed1i \u01b0u \u0111\u1ec3 m\u00f4 h\u00ecnh nh\u1ecf hi\u1ec3u \u0111\u01b0\u1ee3c\n", "- K\u1ebft qu\u1ea3 c\u00f3 th\u1ec3 c\u00f2n h\u1ea1n ch\u1ebf do dung l\u01b0\u1ee3ng m\u00f4 h\u00ecnh nh\u1ecf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udde9 C\u00e0i \u0111\u1eb7t th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft\n", "!pip install transformers accelerate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udce6 N\u1ea1p m\u00f4 h\u00ecnh Qwen 1.5-0.5B-Chat\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "import torch\n", "\n", "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n", "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n", "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n", "model.eval()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udde0 \u0110\u1ecbnh ngh\u0129a h\u00e0m sinh ph\u1ea3n h\u1ed3i v\u1edbi prompt \u0111\u01b0\u1ee3c \u0111i\u1ec1u ch\u1ec9nh\n", "def generate_meaning(term):\n", "    prompt = f\"\"\"\n", "    B\u1ea1n l\u00e0 m\u1ed9t tr\u1ee3 l\u00fd AI chuy\u00ean gi\u1ea3i th\u00edch ti\u1ebfng l\u00f3ng c\u1ee7a gi\u1edbi tr\u1ebb Vi\u1ec7t Nam.\n", "    H\u00e3y gi\u00fap t\u00f4i gi\u1ea3i ngh\u0129a t\u1eeb sau m\u1ed9t c\u00e1ch \u0111\u01a1n gi\u1ea3n, r\u00f5 r\u00e0ng, d\u1ec5 hi\u1ec3u v\u00e0 \u0111\u01b0a ra v\u00ed d\u1ee5 minh h\u1ecda trong c\u00e2u n\u00f3i h\u1eb1ng ng\u00e0y.\n", "    \n", "    T\u1eeb c\u1ea7n gi\u1ea3i th\u00edch: '{term}'\n", "    \"\"\"\n", "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n", "    with torch.no_grad():\n", "        outputs = model.generate(**inputs, max_new_tokens=200)\n", "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u25b6\ufe0f Nh\u1eadp t\u1eeb l\u00f3ng v\u00e0 in k\u1ebft qu\u1ea3 gi\u1ea3i ngh\u0129a\n", "print(generate_meaning(\"\u0111u trend\"))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}